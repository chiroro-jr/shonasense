{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a648ca4c-b918-43c6-93cb-65264d8453d6",
   "metadata": {},
   "source": [
    "# Build a model with it's own embedding layer\n",
    "The prepared clean data can now be used to build a language model. The model will have the following characteristics:\n",
    "- It uses a distributed respresentation for words so that words with similar meanings have a similar representation.\n",
    "- It leans the representation at the time as the learning model.\n",
    "- It learns to predict the probabability for the next word using the context of the last 100 words.\n",
    "\n",
    "Specifically, I will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2372210-dacd-4995-bb4b-9722ca2fdda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"../datasets\"\n",
    "MODELS_DIR = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21ca403-4c0c-43bc-86a4-bd93a5e05caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mufundisi wekumakokoba kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi',\n",
       " 'wekumakokoba kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake',\n",
       " 'kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake zvinonzi',\n",
       " 'akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake zvinonzi mzizi']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r')\n",
    "\n",
    "  # read all the text\n",
    "  text = file.read()\n",
    "\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "in_filename = f\"{DATASETS_DIR}/content_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0dfac-9789-4899-bbc3-84667507ad32",
   "metadata": {},
   "source": [
    "## Encode sequences with using `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94fb144a-fa0c-480f-904f-d3b0bab5a0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open(f'{MODELS_DIR}/tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c7b8fd-2ada-4c38-aeb8-bdb879ba4e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59c28f-28d1-425a-a6af-2bb22b1ff41d",
   "metadata": {},
   "source": [
    "## Sequence inputs and output\n",
    "Now that there are encoded input sequences, they need to be seperated into input($X$) and output($y$). This can be done via array slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43755a06-43ad-44e0-a033-e29464e5e39b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seperate into input and output\n",
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c467e1d2-98ed-4074-b86d-e1babfdfa44b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30430, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68979591-4405-4959-8a30-59a8d5e75799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30430,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2a514-23e2-4eb0-aad1-c3d34038c794",
   "metadata": {},
   "source": [
    "After seperating, each output word needs to be one hot encoded. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the of the words integer value. This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4766bffe-3b15-4195-bd0a-2743e9f5da03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the output word\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff557743-10b2-4eb3-bfc8-345b71eb06ac",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eee61b5b-638d-446d-800b-e93121b1e7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define the model\n",
    "def define_model_with_embedding(vocab_size, seq_length, summary=True):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "  model.add(Bidirectional(LSTM(100)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(100, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "  # compile network\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # summarize the model\n",
    "  if summary:\n",
    "    model.summary()\n",
    "    model_img_path = f'{MODELS_DIR}/model_with_embedding.png'\n",
    "    plot_model(model, to_file=model_img_path, show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8741b438-1b47-4249-84ea-61c72a679223",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 50, 100)           1089800   \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirect  (None, 200)               160800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10898)             1100698   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2371398 (9.05 MB)\n",
      "Trainable params: 2371398 (9.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_with_embedding = define_model_with_embedding(vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07fc6108-a314-4445-bc58-032b30008564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "238/238 [==============================] - 63s 245ms/step - loss: 8.7143 - accuracy: 0.0219\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 63s 264ms/step - loss: 8.3321 - accuracy: 0.0226\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 61s 257ms/step - loss: 8.1598 - accuracy: 0.0227\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 7.9702 - accuracy: 0.0226\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 63s 266ms/step - loss: 7.8198 - accuracy: 0.0222\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 64s 271ms/step - loss: 7.6734 - accuracy: 0.0220\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 61s 258ms/step - loss: 7.5462 - accuracy: 0.0218\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 64s 269ms/step - loss: 7.4090 - accuracy: 0.0228\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 64s 270ms/step - loss: 7.2910 - accuracy: 0.0239\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 63s 265ms/step - loss: 7.1376 - accuracy: 0.0266\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 63s 263ms/step - loss: 7.0403 - accuracy: 0.0291\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 63s 266ms/step - loss: 7.0009 - accuracy: 0.0334\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 6.8030 - accuracy: 0.0374\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 59s 249ms/step - loss: 6.6387 - accuracy: 0.0452\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 6.4632 - accuracy: 0.0513\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 59s 249ms/step - loss: 6.2926 - accuracy: 0.0580\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 6.1108 - accuracy: 0.0693\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 59s 249ms/step - loss: 5.9222 - accuracy: 0.0764\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 5.7177 - accuracy: 0.0901\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 59s 246ms/step - loss: 5.5216 - accuracy: 0.1014\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 59s 248ms/step - loss: 5.3271 - accuracy: 0.1149\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 5.1535 - accuracy: 0.1264\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 59s 248ms/step - loss: 4.9683 - accuracy: 0.1451\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 4.7502 - accuracy: 0.1599\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 59s 250ms/step - loss: 4.5323 - accuracy: 0.1832\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 4.3654 - accuracy: 0.1961\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 4.1655 - accuracy: 0.2205\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 3.9822 - accuracy: 0.2403\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 3.8059 - accuracy: 0.2574\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 59s 250ms/step - loss: 3.6511 - accuracy: 0.2825\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 60s 250ms/step - loss: 3.5031 - accuracy: 0.3033\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 59s 247ms/step - loss: 3.3620 - accuracy: 0.3206\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 60s 251ms/step - loss: 3.2316 - accuracy: 0.3418\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 59s 249ms/step - loss: 3.0759 - accuracy: 0.3639\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 60s 251ms/step - loss: 2.9634 - accuracy: 0.3819\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 2.8464 - accuracy: 0.4020\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 60s 251ms/step - loss: 2.7263 - accuracy: 0.4228\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 60s 251ms/step - loss: 2.6262 - accuracy: 0.4368\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 60s 251ms/step - loss: 2.7258 - accuracy: 0.4341\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 2.5410 - accuracy: 0.4573\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 2.4295 - accuracy: 0.4718\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 60s 254ms/step - loss: 2.3019 - accuracy: 0.4909\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 2.2034 - accuracy: 0.5068\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 2.0974 - accuracy: 0.5259\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 2.0094 - accuracy: 0.5407\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 61s 258ms/step - loss: 1.9229 - accuracy: 0.5590\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 1.8754 - accuracy: 0.5651\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 61s 257ms/step - loss: 1.7727 - accuracy: 0.5868\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 1.7050 - accuracy: 0.5969\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 61s 254ms/step - loss: 1.6470 - accuracy: 0.6102\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 60s 254ms/step - loss: 1.5835 - accuracy: 0.6207\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 1.5227 - accuracy: 0.6337\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 1.4520 - accuracy: 0.6458\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 1.4014 - accuracy: 0.6568\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 61s 254ms/step - loss: 1.3428 - accuracy: 0.6687\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 60s 253ms/step - loss: 1.3111 - accuracy: 0.6743\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 60s 254ms/step - loss: 1.2750 - accuracy: 0.6835\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 1.2247 - accuracy: 0.6964\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 60s 252ms/step - loss: 1.1719 - accuracy: 0.7008\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 61s 258ms/step - loss: 1.1614 - accuracy: 0.7099\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 1.1072 - accuracy: 0.7212\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 1.0730 - accuracy: 0.7272\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 61s 258ms/step - loss: 1.0494 - accuracy: 0.7343\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 1.0135 - accuracy: 0.7360\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.9889 - accuracy: 0.7440\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.9483 - accuracy: 0.7518\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.9269 - accuracy: 0.7584\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 61s 254ms/step - loss: 0.8989 - accuracy: 0.7644\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.8986 - accuracy: 0.7672\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.8839 - accuracy: 0.7683\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.8386 - accuracy: 0.7798\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.8354 - accuracy: 0.7773\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.8014 - accuracy: 0.7881\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.7842 - accuracy: 0.7916\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 62s 260ms/step - loss: 0.7701 - accuracy: 0.7967\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.7497 - accuracy: 0.8016\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.7346 - accuracy: 0.8047\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 61s 257ms/step - loss: 0.7240 - accuracy: 0.8075\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6954 - accuracy: 0.8143\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6936 - accuracy: 0.8126\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6798 - accuracy: 0.8192\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6637 - accuracy: 0.8201\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.6559 - accuracy: 0.8232\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6384 - accuracy: 0.8287\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.6369 - accuracy: 0.8269\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 61s 257ms/step - loss: 0.6332 - accuracy: 0.8274\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5982 - accuracy: 0.8390\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.5935 - accuracy: 0.8395\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5835 - accuracy: 0.8416\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 62s 260ms/step - loss: 0.5550 - accuracy: 0.8474\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.5599 - accuracy: 0.8474\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5588 - accuracy: 0.8482\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 61s 258ms/step - loss: 0.5464 - accuracy: 0.8511\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5419 - accuracy: 0.8514\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5106 - accuracy: 0.8593\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5208 - accuracy: 0.8570\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.5028 - accuracy: 0.8628\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.5107 - accuracy: 0.8592\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 61s 255ms/step - loss: 0.4854 - accuracy: 0.8662\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 61s 256ms/step - loss: 0.4864 - accuracy: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x275543d5a10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_with_embedding.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7828b510-e73c-415d-8915-753c61257577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save the model to file\n",
    "model_with_embedding.save(f'{MODELS_DIR}/model_with_embedding.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf1b20a2-9433-4e4e-8914-b52ab032eb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make new predictions\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "  result = list()\n",
    "  in_text = seed_text\n",
    "  # generate a fixed number of words\n",
    "  for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    # truncate sequences to a fixed length\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # predict probabilities for each word\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # map predicted word index to word\n",
    "    predicted_word_index = np.argmax(yhat)  # Find the index with the highest probability\n",
    "    out_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            out_word = word\n",
    "            break\n",
    "    # append to input\n",
    "    in_text += ' ' + out_word\n",
    "    result.append(out_word)\n",
    "  return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c11b4e1c-2b69-42d1-bd77-eaf4cff55b78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndinonzwa mhepo inondifuridza muviri wose ndichishaya vanodaro mudzimai uyu anotiwo anonzwa achisandudzirwa nekumashure nemunhu waasingaone zvekutopotsa apunzikira pasi pamusoro pezvo anopindwa nemwenje wakanyanya mumaziso ake achipindurwa zvakare zvekutadza kuona iri richitanga ndakatanga kunzwa kupiswa zvakanyanya ndikatokumura mbatya nekudirwa mvura nevanhu ndava kuda kufenda izvozvi nyoka iyi iyo yakareba chipo chemamita maviri\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1156b648-88a8-4a7d-82ef-244f02064b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kuva nebasa chinhu chakanaka kwazvo nokuti basa rinokubatsira kuita [START] asi panguva imwe chete kuti huchange dambudziko iri kuti sei [END]\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"kuva nebasa chinhu chakanaka kwazvo nokuti basa rinokubatsira kuita\"\n",
    "pred = generate_seq(model_with_embedding, tokenizer, seq_length, seed_text, 10)\n",
    "print(seed_text + ' [START] ' + pred + ' [END]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503a104-2414-4a3f-92ed-f02c38ba02db",
   "metadata": {},
   "source": [
    "## Training with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fae0d2a0-c59a-45c9-ab78-c785f18dd4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce97903b-a5d5-4b2d-8b89-e75883beecbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d57212-9a1e-483e-899e-0ef3a2b0ab9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # compile the model\n",
    "    model_with_embedding = define_model_with_embedding(vocab_size, seq_length, summary=False)\n",
    "    \n",
    "    # Fit the model\n",
    "    model_with_embedding.fit(X[train], y[train], epochs=20, batch_size=128)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model_with_embedding.evaluate(X[test], y[test])\n",
    "    print(\"Accuracy = %s: %.2f%%\" % (model_with_embedding.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"Mean Accuracy = %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
