{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3362d49f-cdc4-4b2e-a1b5-72ab01e3bba5",
   "metadata": {},
   "source": [
    "# Build a model with Gensim word embeddings\n",
    "This will be mostly the same as the other model but instead of having it's own embedding layer it will contain embeddings from Gensim's `Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336514ec-c48d-4d3f-955f-60de18bfcf7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"../datasets\"\n",
    "MODELS_DIR = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d695326e-8bd0-41c2-a145-11d49c04e6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mufundisi wekumakokoba kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi',\n",
       " 'wekumakokoba kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake',\n",
       " 'kubulawayo akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake zvinonzi',\n",
       " 'akashamisa vagari venzvimbo iyi apo akarova muroja wake uyo anove asina kupfeka sechirango chekuridza mimhanzi zvine ruzha rwakanyanyisa apo iye aive pakati pekuitisa svondo mufundisi uye muvambi wechechi yetsime rase birthsider pastor khumbulani mzizi uyo aiitisa chechi pamba pake akarova sikhululiwe dube apo airidza mimhanzi zvine ruzha pawairesi yake zvinonzi mzizi']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r')\n",
    "\n",
    "  # read all the text\n",
    "  text = file.read()\n",
    "\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "in_filename = f\"{DATASETS_DIR}/content_sequences.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2ba98-e859-46b3-8933-fb5f815010d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encode sequences with using `Word2Vec` word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42129a1f-d535-44f3-999a-65179fcc3615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to encode text sequences using Word2Vec word index\n",
    "# instead of Tokenizer's word index\n",
    "# This is because one of the models will use external word embeddings\n",
    "def encode_text_with_word2vec(input_text, model, oov_token=\"<OOV>\"):\n",
    "  # OOV configuration\n",
    "  oov_index = -1\n",
    "  oov_token = \"<OOV>\"\n",
    "\n",
    "  # list of encoded sequences\n",
    "  sequences = list()\n",
    "\n",
    "  for text in input_text:\n",
    "    # split the text into words (Tokenize it)\n",
    "    words = text.split()\n",
    "    sequence = list()\n",
    "\n",
    "    # map words to indeces using Word2Vec's word index\n",
    "    for word in words:\n",
    "      if word in model.wv.key_to_index:\n",
    "        sequence.append(model.wv.key_to_index[word])\n",
    "      else:\n",
    "        # Handle OOV words by mapping them to a special token (oov_token in this case)\n",
    "        sequence.append(model.wv.key_to_index.get(oov_token,0)) # 0 for unknown words\n",
    "\n",
    "    sequences.append(sequence)\n",
    "\n",
    "  return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a826d1-7e09-4e25-b3a0-9f2a30051e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load Word2Vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec.load(f\"{MODELS_DIR}/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a31d686-a98d-43f6-9644-1f8b08a2ed38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequences = encode_text_with_word2vec(lines, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cd6b2-3ef6-4d2c-9526-a8154512b306",
   "metadata": {},
   "source": [
    "## Sequence inputs and output\n",
    "Now that there are encoded input sequences, they need to be seperated into input($X$) and output($y$). This can be done via array slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1534be0-ff64-46da-b341-9ceaac2eb24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "067ab3ce-dc35-4b85-8c78-1439548bf0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30430, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d66cab3-864f-4a0e-bb2f-176eec7b3ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30430,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e79b4-02b8-4e5d-9116-b1fecd7fc50b",
   "metadata": {},
   "source": [
    "After seperating, each output word needs to be one hot encoded. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the of the words integer value. This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d25d4b-2b67-42ee-af72-598209e3dfcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the output word\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "vocab_size = len(w2v_model.wv.key_to_index)\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9148bae-34cb-4fc3-95ed-2bef2ddd5fcd",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d6f0098-4b50-4bea-b20c-b5cf35be6ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define the model\n",
    "def define_model_with_embedding(vocab_size, seq_length, summary=True):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(len(w2v_model.wv.key_to_index), 100, weights=[w2v_model.wv.vectors], trainable=False))\n",
    "  model.add(Bidirectional(LSTM(100)))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(100, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "  # compile network\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # summarize the model\n",
    "  if summary:\n",
    "    model.summary()\n",
    "    model_img_path = f'{MODELS_DIR}/model_with_gensim_embedding.png'\n",
    "    plot_model(model, to_file=model_img_path, show_shapes=True)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03434e71-63e8-4b2f-93d1-576c1a673bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, None, 100)         1089700   \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirecti  (None, 200)               160800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10897)             1100597   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2371197 (9.05 MB)\n",
      "Trainable params: 1281497 (4.89 MB)\n",
      "Non-trainable params: 1089700 (4.16 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model_with_gensim_embedding = define_model_with_embedding(vocab_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7d3e58e-5801-4997-ae58-97a1d7a8b5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "238/238 [==============================] - 47s 175ms/step - loss: 8.6316 - accuracy: 0.0219\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 42s 174ms/step - loss: 7.9931 - accuracy: 0.0300\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 7.5568 - accuracy: 0.0411\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 40s 170ms/step - loss: 7.1347 - accuracy: 0.0504\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 41s 170ms/step - loss: 6.6623 - accuracy: 0.0592\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 41s 171ms/step - loss: 6.1146 - accuracy: 0.0694\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 41s 171ms/step - loss: 5.4800 - accuracy: 0.0840\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 4.8292 - accuracy: 0.1081\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 4.2469 - accuracy: 0.1563\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 3.7988 - accuracy: 0.2165\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 3.4941 - accuracy: 0.2651\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 49s 205ms/step - loss: 3.2349 - accuracy: 0.3044\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 53s 221ms/step - loss: 3.0501 - accuracy: 0.3335\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 2.8968 - accuracy: 0.3568\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 2.7693 - accuracy: 0.3796\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 2.6499 - accuracy: 0.3965\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 2.5433 - accuracy: 0.4173\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 2.4585 - accuracy: 0.4275\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 2.3676 - accuracy: 0.4449\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 2.2889 - accuracy: 0.4589\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 45s 188ms/step - loss: 2.2290 - accuracy: 0.4703\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 49s 207ms/step - loss: 2.1535 - accuracy: 0.4839\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 43s 181ms/step - loss: 2.0932 - accuracy: 0.4957\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 43s 180ms/step - loss: 2.0260 - accuracy: 0.5083\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 43s 181ms/step - loss: 1.9898 - accuracy: 0.5141\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.9458 - accuracy: 0.5225\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 42s 178ms/step - loss: 1.8914 - accuracy: 0.5332\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 42s 178ms/step - loss: 1.8437 - accuracy: 0.5418\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 42s 179ms/step - loss: 1.8018 - accuracy: 0.5532\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.7513 - accuracy: 0.5608\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.7288 - accuracy: 0.5656\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 43s 182ms/step - loss: 1.6780 - accuracy: 0.5740\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 42s 177ms/step - loss: 1.6480 - accuracy: 0.5802\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 42s 177ms/step - loss: 1.6176 - accuracy: 0.5884\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.5812 - accuracy: 0.5964\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 43s 180ms/step - loss: 1.5627 - accuracy: 0.5988\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.5169 - accuracy: 0.6094\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 42s 176ms/step - loss: 1.5101 - accuracy: 0.6094\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 42s 177ms/step - loss: 1.4727 - accuracy: 0.6176\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.4359 - accuracy: 0.6248\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 42s 177ms/step - loss: 1.4221 - accuracy: 0.6275\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 42s 176ms/step - loss: 1.3988 - accuracy: 0.6360\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 43s 183ms/step - loss: 1.3679 - accuracy: 0.6393\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 43s 179ms/step - loss: 1.3452 - accuracy: 0.6440\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 45s 190ms/step - loss: 1.3183 - accuracy: 0.6526\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 45s 187ms/step - loss: 1.3108 - accuracy: 0.6493\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.2791 - accuracy: 0.6581\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 40s 167ms/step - loss: 1.2738 - accuracy: 0.6621\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.2572 - accuracy: 0.6623\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.2276 - accuracy: 0.6691\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 40s 167ms/step - loss: 1.2109 - accuracy: 0.6734\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 40s 166ms/step - loss: 1.1959 - accuracy: 0.6762\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.1745 - accuracy: 0.6842\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 41s 170ms/step - loss: 1.1666 - accuracy: 0.6827\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 40s 167ms/step - loss: 1.1403 - accuracy: 0.6879\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.1442 - accuracy: 0.6889\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.1266 - accuracy: 0.6948\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 1.1078 - accuracy: 0.6964\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 1.0987 - accuracy: 0.6976\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.0816 - accuracy: 0.7043\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 41s 171ms/step - loss: 1.0497 - accuracy: 0.7113\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 1.0518 - accuracy: 0.7116\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 40s 168ms/step - loss: 1.0363 - accuracy: 0.7149\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 1.0271 - accuracy: 0.7172\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 1.0272 - accuracy: 0.7139\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 1.0110 - accuracy: 0.7181\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.9847 - accuracy: 0.7261\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.9616 - accuracy: 0.7314\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 41s 171ms/step - loss: 0.9692 - accuracy: 0.7309\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.9582 - accuracy: 0.7341\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.9561 - accuracy: 0.7321\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 42s 174ms/step - loss: 0.9386 - accuracy: 0.7354\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.9362 - accuracy: 0.7389\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.9165 - accuracy: 0.7423\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.9158 - accuracy: 0.7398\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.9115 - accuracy: 0.7442\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 42s 174ms/step - loss: 0.8882 - accuracy: 0.7488\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.8823 - accuracy: 0.7502\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.8706 - accuracy: 0.7545\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.8634 - accuracy: 0.7542\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.8604 - accuracy: 0.7539\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.8406 - accuracy: 0.7583\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.8443 - accuracy: 0.7599\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.8474 - accuracy: 0.7601\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.8249 - accuracy: 0.7631\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.8179 - accuracy: 0.7655\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 43s 180ms/step - loss: 0.8074 - accuracy: 0.7704\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 42s 177ms/step - loss: 0.7987 - accuracy: 0.7681\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.7969 - accuracy: 0.7685\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.7849 - accuracy: 0.7744\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 42s 175ms/step - loss: 0.7834 - accuracy: 0.7721\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 42s 175ms/step - loss: 0.7864 - accuracy: 0.7741\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 42s 175ms/step - loss: 0.7817 - accuracy: 0.7719\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 42s 175ms/step - loss: 0.7906 - accuracy: 0.7731\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.7728 - accuracy: 0.7781\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 41s 174ms/step - loss: 0.7656 - accuracy: 0.7781\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.7555 - accuracy: 0.7805\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.7523 - accuracy: 0.7825\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 41s 173ms/step - loss: 0.7474 - accuracy: 0.7839\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 41s 172ms/step - loss: 0.7308 - accuracy: 0.7883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ed77a27350>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model_with_gensim_embedding.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d23155b2-4037-4653-96b4-b283d9958365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "# save the model to file\n",
    "model_with_gensim_embedding.save(f'{MODELS_DIR}/model_with_gensim_embedding.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391100f-b633-4ecc-b3a0-016e8e965ac5",
   "metadata": {},
   "source": [
    "## Training with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe9a7cd1-22f0-4256-b006-2ebe306b7ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e7bf776-dc94-4518-ae70-fcf955cfc224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176d127-4287-47c4-8302-eb6a82f0d5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, y):\n",
    "    # compile the model\n",
    "    model_with_embedding = define_model_with_embedding(vocab_size, seq_length, summary=False)\n",
    "    \n",
    "    # Fit the model\n",
    "    model_with_embedding.fit(X[train], y[train], epochs=20, batch_size=128)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model_with_embedding.evaluate(X[test], y[test])\n",
    "    print(\"Accuracy = %s: %.2f%%\" % (model_with_embedding.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"Mean Accuracy = %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
