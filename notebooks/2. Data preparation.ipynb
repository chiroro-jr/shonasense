{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eebe5e6-6690-48e9-bc42-2a0fb33b37c3",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "I will be using text from [Kwayedza Newspaper](https://kwayedza.co.zw) which I scrapped into [content.txt](../datasets/content.txt). The code for this can be found in `1. Getting the data.ipynb` notebook.\n",
    "\n",
    "Here a sample of what the text in `content.txt` looks like:\n",
    "```bash\n",
    "SANGANO reZimbabwe Indigenous Women Farmers Trust (ZIWFT) rakatanga\n",
    "chirongwa chekudzidzisa varimi kugadzira fetireza pachishandiswa\n",
    "zviwanikwa zvemunharaunda dzavo (organic fertiliser) sezvo mhando iyi\n",
    "isingadhure mukuigadzira uye ichiwanisa chikafu chisina njodzi kuutano\n",
    "hweveruzhinji.\n",
    "```\n",
    "I will generate a model of this text that I can then use to generate new sequences of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ddb649-7dbd-4ea7-bc64-ad321454623c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6fe38d-53d3-40dc-8f2a-52dcf33ce115",
   "metadata": {},
   "source": [
    "## Load the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074b92d9-f612-43a3-882a-e95ed619b6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to load the text document into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r', encoding='utf-8')\n",
    "\n",
    "  # read all the text in the file\n",
    "  text = file.read()\n",
    "\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec44930-ad10-42de-82fa-34bd067ac44f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREMIER Soccer League (PSL) iri kudya magaka mambishi zvichitevera mhirizhonga yakaitika kuBabourfields, kuBulawayo nezuro apo vatsigiri veHighlanders vakapinda munhandare ndokutanga kukanda matombo v'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = f\"{DATASETS_DIR}/content.txt\"\n",
    "doc = load_doc(in_filename)\n",
    "doc[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89138a03-8798-4416-a07b-c71fb926c354",
   "metadata": {},
   "source": [
    "### Clean the text\n",
    "The text needs to be transforemed into a sequence of tokens of words that we can use as a source to train the model. But before that there are some operations that need to be performed to clean the text:\n",
    "- replace `-` with a whitespace so we can split words better\n",
    "- split words based on whitespace\n",
    "- remove all punctuation from the words to reduce the vocabulary size\n",
    "- remove all words that are not alphabetic to remove standalone punctuation tokens\n",
    "- normalize all words to lowercase toreduce the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6d2500-dbd9-44b0-8c6e-97929b5b216f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "  # replace '--' with a space ' '\n",
    "  doc = doc.replace('--', ' ')\n",
    "\n",
    "  # split into tokens by white space\n",
    "  tokens = doc.split()\n",
    "\n",
    "  # prepare regex for char filtering\n",
    "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "  # remove punctuation from each word\n",
    "  tokens = [re_punc.sub('', w) for w in tokens]\n",
    "\n",
    "  # remove remaining tokens that are not alphabetic\n",
    "  tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "  # make lower case\n",
    "  tokens = [word.lower() for word in tokens]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e237aa6f-9364-4648-893e-aaf33ed560aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['premier', 'soccer', 'league', 'psl', 'iri', 'kudya', 'magaka', 'mambishi', 'zvichitevera', 'mhirizhonga', 'yakaitika', 'kubabourfields', 'kubulawayo', 'nezuro', 'apo', 'vatsigiri', 'vehighlanders', 'vakapinda', 'munhandare', 'ndokutanga', 'kukanda', 'matombo', 'vachirwisana', 'nemapurisa', 'vachinyunyuta', 'kuti', 'chikwata', 'chavo', 'change', 'chanyimwa', 'pena', 'mutambo', 'wecastle', 'premier', 'soccer', 'league', 'uyu', 'waive', 'pakati', 'pedynamos', 'fc', 'nehighlanders', 'wakazomiswa', 'nekuda', 'kwemhirizhonga', 'yakatangiswa', 'nevatsigiri', 'vehighlanders', 'watambwa', 'maminitsi', 'apo', 'dembare', 'yaitungamira', 'mhirizhonga', 'iyi', 'yakaona', 'vamwe', 'vatsigiri', 'venhabvu', 'vachikuvara', 'zvakaipisisa', 'muchinyorwa', 'sachigaro', 'wepsl', 'farai', 'jere', 'anoti', 'psl', 'inoshora', 'nyaya', 'dzemhizhonga', 'dzinoitika', 'munhabvu', 'uye', 'vari', 'kuongorora', 'chiitiko', 'chekubulawayo', 'soccer', 'league', 'inoshora', 'zvikuru', 'nyaya', 'yemhirizhonga', 'yakaitika', 'pamutambo', 'wedynamos', 'nehighlanders', 'kubabourfields', 'stadium', 'nemusi', 'wesvondo', 'gunyana', 'kunyorerwa', 'kunzwa', 'kuti', 'zvakafamba', 'sei', 'namatch', 'commissioner', 'pamwe', 'chete', 'nemuridzi', 'wepembe', 'wemutambo', 'uyu', 'tisati', 'tatanga', 'kutora', 'matanho', 'ekuranga', 'tinoda', 'kuzivisa', 'kuti', 'mhirizhonga', 'nehunhubu', 'hazvina', 'nzvimbo', 'munhabvu', 'yedu', 'uye', 'tinotarisira', 'kuti', 'vanotyora', 'mutemo', 'anodaro', 'jere', 'anoti', 'nhabvu', 'hainei', 'nezvematongerwo', 'enyika', 'asi', 'iripo', 'pakubatanidza', 'vanhu', 'vanobva', 'munharaunda', 'dzakasiyana', 'isangano', 'risinei', 'nezvematongerwo', 'enyika', 'asi', 'kuti', 'kusimudzira', 'nhabvu', 'nekubatanidza', 'nharaunda', 'dzakasiyana', 'tinoshora', 'nhubu', 'dzinoda', 'kushandisa', 'kuungana', 'munhabvu', 'kuita', 'zvisirizvo', 'zvehumbimbindoga', 'nekupatsanura', 'vanhu', 'zvikwata', 'kuti', 'zvidzidzise', 'vatsigiri', 'vazvo', 'mitemo', 'yenhabvu', 'mukana', 'uyu', 'kukumbira', 'ruregerero', 'kusports', 'and', 'recreation', 'commission', 'yakamirira', 'hurumende', 'yezimbabwe', 'zifa', 'vakotsveri', 'vatsigiri', 'nevamwe', 'vane', 'chekuita', 'anodaro', 'jere', 'zvakadaro', 'highlanders', 'inoti', 'yakashushikana', 'nemhirizhonga', 'yakaitika', 'pamutambo', 'uyu', 'football', 'club', 'takashushikana', 'zvikuru', 'nemhirizhonga']\n",
      "Total Tokens: 5509\n",
      "Unique Tokens: 2765\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a16bbc-d174-468a-b3fb-404d0d0cc89e",
   "metadata": {},
   "source": [
    "### Save the clean text\n",
    "Tokens can be organized into sequences of 50 input words and 1 output word i.e. sequences of 51 words. This can be done by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens. The tokens will be transformed into space-seperated strings for later storage in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c97af75-0647-449a-b474-7eb1f7a297b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5498\n"
     ]
    }
   ],
   "source": [
    "length = 10 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "  # select a sequence of tokens\n",
    "  seq = tokens[i - length: i]\n",
    "\n",
    "  # convert the sequence into a line\n",
    "  line = ' '.join(seq)\n",
    "\n",
    "  # store the line\n",
    "  sequences.append(line)\n",
    "\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be7e8a9-4aa6-4a8e-b651-2395470fedd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line (checkpoint)\n",
    "def save_doc(lines, filename):\n",
    "  data = '\\n'.join(lines)\n",
    "  file = open(filename, 'w', encoding='utf-8')\n",
    "  file.write(data)\n",
    "  file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe111097-0804-4734-8157-7b19df7e47ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save sequences to file (checkpoint)\n",
    "out_filename = f\"{DATASETS_DIR}/content_sequences.txt\"\n",
    "save_doc(sequences, out_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
